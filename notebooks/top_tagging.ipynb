{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rd804/particle_transformer')\n",
    "import networks.example_ParticleTransformer as part\n",
    "import networks.example_ParticleTransformer_finetune as part_finetune\n",
    "from weaver.utils.dataset import DataConfig\n",
    "import torch\n",
    "from weaver.nn.model.ParticleTransformer import ParticleTransformer\n",
    "from weaver.utils.logger import _logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rd804/particle_transformer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "import h5py\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config_file = 'data/JetClass/JetClass_kin.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig.load(data_config_file, load_observers=False, load_reweight_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_features = torch.from_numpy(np.load('data/part_features.npy'))\n",
    "part_4momenta = np.load('data/part_4momenta.npy')\n",
    "part_labels = torch.from_numpy(np.load('data/part_labels.npy'))\n",
    "\n",
    "mask = torch.tensor((part_4momenta!=[0,0,0,0])[...,0]).unsqueeze(-1)\n",
    "\n",
    "part_4momenta = torch.tensor(part_4momenta).to(device)\n",
    "vector = torch.swapaxes(part_4momenta, 1, 2).to(device)\n",
    "features = torch.swapaxes(part_features, 1, 2).to(device)\n",
    "mask = torch.swapaxes(mask, 1, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of parameters\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 7\n",
      "The model has 2,142,166 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, model_info = part.get_model(data_config)\n",
    "model.to(device)\n",
    "#model.mod.requires_grad_(False)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "path_part = 'models/ParT_kin.pt'\n",
    "model.load_state_dict(torch.load(path_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd804/.conda/envs/part/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 done\n",
      "Batch 1 done\n",
      "Batch 2 done\n",
      "Batch 3 done\n",
      "Batch 4 done\n",
      "Batch 5 done\n",
      "Batch 6 done\n",
      "Batch 7 done\n",
      "Batch 8 done\n",
      "Batch 9 done\n",
      "Batch 10 done\n",
      "Batch 11 done\n",
      "Batch 12 done\n",
      "Batch 13 done\n",
      "Batch 14 done\n",
      "Batch 15 done\n",
      "Batch 16 done\n",
      "Batch 17 done\n",
      "Batch 18 done\n",
      "Batch 19 done\n",
      "Batch 20 done\n",
      "Batch 21 done\n",
      "Batch 22 done\n",
      "Batch 23 done\n",
      "Batch 24 done\n",
      "Batch 25 done\n",
      "Batch 26 done\n",
      "Batch 27 done\n",
      "Batch 28 done\n",
      "Batch 29 done\n",
      "Batch 30 done\n",
      "Batch 31 done\n",
      "Batch 32 done\n",
      "Batch 33 done\n",
      "Batch 34 done\n",
      "Batch 35 done\n",
      "Batch 36 done\n",
      "Batch 37 done\n",
      "Batch 38 done\n",
      "Batch 39 done\n",
      "Batch 40 done\n",
      "Batch 41 done\n",
      "Batch 42 done\n",
      "Batch 43 done\n",
      "Batch 44 done\n",
      "Batch 45 done\n",
      "Batch 46 done\n",
      "Batch 47 done\n",
      "Batch 48 done\n",
      "Batch 49 done\n",
      "Batch 50 done\n",
      "Batch 51 done\n",
      "Batch 52 done\n",
      "Batch 53 done\n",
      "Batch 54 done\n",
      "Batch 55 done\n",
      "Batch 56 done\n",
      "Batch 57 done\n",
      "Batch 58 done\n",
      "Batch 59 done\n",
      "Batch 60 done\n",
      "Batch 61 done\n",
      "Batch 62 done\n",
      "Batch 63 done\n",
      "Batch 64 done\n",
      "Batch 65 done\n",
      "Batch 66 done\n",
      "Batch 67 done\n",
      "Batch 68 done\n",
      "Batch 69 done\n",
      "Batch 70 done\n",
      "Batch 71 done\n",
      "Batch 72 done\n",
      "Batch 73 done\n",
      "Batch 74 done\n",
      "Batch 75 done\n",
      "Batch 76 done\n",
      "Batch 77 done\n",
      "Batch 78 done\n",
      "Batch 79 done\n",
      "Batch 80 done\n",
      "Batch 81 done\n",
      "Batch 82 done\n",
      "Batch 83 done\n",
      "Batch 84 done\n",
      "Batch 85 done\n",
      "Batch 86 done\n",
      "Batch 87 done\n",
      "Batch 88 done\n",
      "Batch 89 done\n",
      "Batch 90 done\n",
      "Batch 91 done\n",
      "Batch 92 done\n",
      "Batch 93 done\n",
      "Batch 94 done\n",
      "Batch 95 done\n",
      "Batch 96 done\n",
      "Batch 97 done\n",
      "Batch 98 done\n",
      "Batch 99 done\n",
      "Batch 100 done\n",
      "Batch 101 done\n",
      "Batch 102 done\n",
      "Batch 103 done\n",
      "Batch 104 done\n",
      "Batch 105 done\n",
      "Batch 106 done\n",
      "Batch 107 done\n",
      "Batch 108 done\n",
      "Batch 109 done\n",
      "Batch 110 done\n",
      "Batch 111 done\n",
      "Batch 112 done\n",
      "Batch 113 done\n",
      "Batch 114 done\n",
      "Batch 115 done\n",
      "Batch 116 done\n",
      "Batch 117 done\n",
      "Batch 118 done\n",
      "Batch 119 done\n",
      "Batch 120 done\n",
      "Batch 121 done\n",
      "Batch 122 done\n",
      "Batch 123 done\n",
      "Batch 124 done\n",
      "Batch 125 done\n",
      "Batch 126 done\n",
      "Batch 127 done\n",
      "Batch 128 done\n",
      "Batch 129 done\n",
      "Batch 130 done\n",
      "Batch 131 done\n",
      "Batch 132 done\n",
      "Batch 133 done\n",
      "Batch 134 done\n",
      "Batch 135 done\n",
      "Batch 136 done\n",
      "Batch 137 done\n",
      "Batch 138 done\n",
      "Batch 139 done\n",
      "Batch 140 done\n",
      "Batch 141 done\n",
      "Batch 142 done\n",
      "Batch 143 done\n",
      "Batch 144 done\n",
      "Batch 145 done\n",
      "Batch 146 done\n",
      "Batch 147 done\n",
      "Batch 148 done\n",
      "Batch 149 done\n",
      "Batch 150 done\n",
      "Batch 151 done\n",
      "Batch 152 done\n",
      "Batch 153 done\n",
      "Batch 154 done\n",
      "Batch 155 done\n",
      "Batch 156 done\n",
      "Batch 157 done\n",
      "Batch 158 done\n",
      "Batch 159 done\n",
      "Batch 160 done\n",
      "Batch 161 done\n",
      "Batch 162 done\n",
      "Batch 163 done\n",
      "Batch 164 done\n",
      "Batch 165 done\n",
      "Batch 166 done\n",
      "Batch 167 done\n",
      "Batch 168 done\n",
      "Batch 169 done\n",
      "Batch 170 done\n",
      "Batch 171 done\n",
      "Batch 172 done\n",
      "Batch 173 done\n",
      "Batch 174 done\n",
      "Batch 175 done\n",
      "Batch 176 done\n",
      "Batch 177 done\n",
      "Batch 178 done\n",
      "Batch 179 done\n",
      "Batch 180 done\n",
      "Batch 181 done\n",
      "Batch 182 done\n",
      "Batch 183 done\n",
      "Batch 184 done\n",
      "Batch 185 done\n",
      "Batch 186 done\n",
      "Batch 187 done\n",
      "Batch 188 done\n",
      "Batch 189 done\n",
      "Batch 190 done\n",
      "Batch 191 done\n",
      "Batch 192 done\n",
      "Batch 193 done\n",
      "Batch 194 done\n",
      "Batch 195 done\n",
      "Batch 196 done\n",
      "Batch 197 done\n",
      "Batch 198 done\n",
      "Batch 199 done\n",
      "Batch 200 done\n",
      "Batch 201 done\n"
     ]
    }
   ],
   "source": [
    "mini_batch = 2000\n",
    "pred= []\n",
    "\n",
    "\n",
    "for i in range(len(features)//mini_batch):\n",
    "    with torch.no_grad():\n",
    "        pred.append(model.forward(features[i*mini_batch:(i+1)*mini_batch], vector[i*mini_batch:(i+1)*mini_batch], mask[i*mini_batch:(i+1)*mini_batch]))\n",
    "    print(f'Batch {i} done')\n",
    "\n",
    "\n",
    "\n",
    "#with torch.no_grad():\n",
    " #   pred = model.forward(features[:100], vector[:100], mask[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.concatenate([p.cpu().numpy() for p in pred], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/pred.npy', ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "def R30(y_true,y_pred,thr): \n",
    "    #to find the threshold that gives TPR of 50%, and the false positive rate at a TPR of 50% \n",
    "    fpr_, tpr_, thresholds_ = roc_curve(y_true, y_pred)\n",
    "\n",
    "    t50 = thresholds_[np.argmin(np.absolute(tpr_-thr))]\n",
    "\n",
    "    fpr50 = fpr_[thresholds_==t50].item()\n",
    "    tpr50 = tpr_[thresholds_==t50].item()\n",
    "    if fpr50>0:\n",
    "        print('R30 at tpr ', tpr50, ' is ',1/fpr50)\n",
    "    \n",
    "    #print(tpr50)\n",
    "    return  t50, fpr50, tpr50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R30 at tpr  0.3000089146864507  is  4.490101539760482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.08696413, 0.22271211266490504, 0.3000089146864507)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R30(part_labels[:,1], ypred[:,1], 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
